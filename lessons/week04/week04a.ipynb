{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"gpuType":"T4","authorship_tag":"ABX9TyOIqSl9PBlaXiTIF3nye8Vi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["\n","<center>\n","<img src=\"https://drive.google.com/uc?id=1B7bif7ps19gO0Ieke_E-sOwSPMkfhWtf\" width=\"80%\">\n","</center>"],"metadata":{"id":"ZeOHYxdGR24y"}},{"cell_type":"markdown","source":["# Import libraries\n","\n"],"metadata":{"id":"wlOAPpZdRURD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GOFCPpgFLF26"},"outputs":[],"source":["import numpy as np\n","from sklearn.linear_model import LinearRegression\n","\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","from torch.utils.data import Dataset, TensorDataset, DataLoader\n","from torch.utils.data.dataset import random_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.style.use('fivethirtyeight')"]},{"cell_type":"markdown","source":["# Data Generation V0\n","\n"],"metadata":{"id":"_LdSC1rEQX7k"}},{"cell_type":"code","source":["import numpy as np\n","\n","true_b = 1\n","true_w = 2\n","N = 100\n","\n","# Data Generation\n","np.random.seed(42)\n","x = np.random.rand(N, 1)\n","y = true_b + true_w * x + (.1 * np.random.randn(N, 1))"],"metadata":{"id":"iqfZUi_GRaHA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Preparation V2"],"metadata":{"id":"1c_PHJ05RfIz"}},{"cell_type":"code","source":["torch.manual_seed(13)\n","\n","# Builds tensors from numpy arrays BEFORE split\n","x_tensor = torch.from_numpy(x).float()\n","y_tensor = torch.from_numpy(y).float()\n","\n","# Builds dataset containing ALL data points\n","dataset = TensorDataset(x_tensor, y_tensor)\n","\n","# Performs the split\n","ratio = .8\n","n_total = len(dataset)\n","n_train = int(n_total * ratio)\n","n_val = n_total - n_train\n","\n","train_data, val_data = random_split(dataset, [n_train, n_val])\n","\n","# Builds a loader of each set\n","train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)\n","val_loader = DataLoader(dataset=val_data, batch_size=16)"],"metadata":{"id":"AMh8NncmSeJE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Configuration V2"],"metadata":{"id":"1VOmkatUSqUe"}},{"cell_type":"markdown","source":["\n","## Helper Function 1"],"metadata":{"id":"Xbx3fSLYUewl"}},{"cell_type":"code","source":["def make_train_step_fn(model, loss_fn, optimizer):\n","    # Builds function that performs a step in the train loop\n","    def perform_train_step_fn(x, y):\n","        # Sets model to TRAIN mode\n","        model.train()\n","\n","        # Step 1 - Computes our model's predicted output - forward pass\n","        yhat = model(x)\n","        # Step 2 - Computes the loss\n","        loss = loss_fn(yhat, y)\n","        # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n","        loss.backward()\n","        # Step 4 - Updates parameters using gradients and the learning rate\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","        # Returns the loss\n","        return loss.item()\n","\n","    # Returns the function that will be called inside the train loop\n","    return perform_train_step_fn"],"metadata":{"id":"ml1Z-5rHUhk1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Helper Function 2"],"metadata":{"id":"Zp2dz7uZUxdS"}},{"cell_type":"code","source":["def make_val_step_fn(model, loss_fn):\n","    # Builds function that performs a step in the validation loop\n","    def perform_val_step_fn(x, y):\n","        # Sets model to EVAL mode\n","        model.eval()\n","\n","        # Step 1 - Computes our model's predicted output - forward pass\n","        yhat = model(x)\n","        # Step 2 - Computes the loss\n","        loss = loss_fn(yhat, y)\n","        # There is no need to compute Steps 3 and 4, since we don't update parameters during evaluation\n","        return loss.item()\n","\n","    return perform_val_step_fn"],"metadata":{"id":"2PksvBV_Uzhk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## All in one"],"metadata":{"id":"eHqBGM8IU6aw"}},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n","lr = 0.1\n","\n","torch.manual_seed(42)\n","# Now we can create a model and send it at once to the device\n","model = nn.Sequential(nn.Linear(1, 1)).to(device)\n","\n","# Defines a SGD optimizer to update the parameters (now retrieved directly from the model)\n","optimizer = optim.SGD(model.parameters(), lr=lr)\n","\n","# Defines a MSE loss function\n","loss_fn = nn.MSELoss(reduction='mean')\n","\n","# Creates the train_step function for our model, loss function and optimizer\n","train_step_fn = make_train_step_fn(model, loss_fn, optimizer)\n","\n","# Creates the val_step function for our model and loss function\n","val_step_fn = make_val_step_fn(model, loss_fn)"],"metadata":{"id":"7cPbz4OjS-Jl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Training V4"],"metadata":{"id":"LQ_hvdJhVbZT"}},{"cell_type":"markdown","source":["## Helper Function 3"],"metadata":{"id":"aQU_0k5CUnMu"}},{"cell_type":"code","source":["def mini_batch(device, data_loader, step_fn):\n","    mini_batch_losses = []\n","    for x_batch, y_batch in data_loader:\n","        x_batch = x_batch.to(device)\n","        y_batch = y_batch.to(device)\n","\n","        mini_batch_loss = step_fn(x_batch, y_batch)\n","        mini_batch_losses.append(mini_batch_loss)\n","\n","    loss = np.mean(mini_batch_losses)\n","    return loss"],"metadata":{"id":"XsyVdi4yUq_s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"OjoYFtY4pCVr"}},{"cell_type":"code","source":["# Defines number of epochs\n","n_epochs = 200\n","\n","losses = []\n","val_losses = []\n","\n","for epoch in range(n_epochs):\n","    # inner loop\n","    loss = mini_batch(device, train_loader, train_step_fn)\n","    losses.append(loss)\n","\n","    # VALIDATION\n","    # no gradients in validation!\n","    with torch.no_grad():\n","        val_loss = mini_batch(device, val_loader, val_step_fn)\n","        val_losses.append(val_loss)"],"metadata":{"id":"WfA4ehMcVmdX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(model.state_dict())"],"metadata":{"id":"EPDDKstsVvGz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = plt.figure(figsize=(10, 4))\n","plt.plot(losses, label='Training Loss', c='b')\n","plt.plot(val_losses, label='Validation Loss', c='r')\n","plt.yscale('log')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.tight_layout()"],"metadata":{"id":"UQZPYi7CVyjh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Saving and Loading Models"],"metadata":{"id":"a4hpIm8KWNif"}},{"cell_type":"markdown","source":["## Saving"],"metadata":{"id":"nYmf_qEMWuiB"}},{"cell_type":"code","source":["checkpoint = {'epoch': n_epochs,\n","              'model_state_dict': model.state_dict(),\n","              'optimizer_state_dict': optimizer.state_dict(),\n","              'loss': losses,\n","              'val_loss': val_losses}\n","\n","torch.save(checkpoint, 'model_checkpoint.pth')"],"metadata":{"id":"VJthjE83WaLg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer.state_dict()"],"metadata":{"id":"hza4Xdf3gLEa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Loading"],"metadata":{"id":"rin0UkxrWfs_"}},{"cell_type":"code","source":["checkpoint = torch.load('model_checkpoint.pth')\n","\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","saved_epoch = checkpoint['epoch']\n","saved_losses = checkpoint['loss']\n","saved_val_losses = checkpoint['val_loss']\n","\n","model.train() # always use TRAIN for resuming training"],"metadata":{"id":"FeNzHOBoWxs2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(model.state_dict())"],"metadata":{"id":"A_3KMmbgWzwO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Deploying / Making Predictions\n"],"metadata":{"id":"VkuQV0hiW2p5"}},{"cell_type":"code","source":["checkpoint = torch.load('model_checkpoint.pth')\n","\n","model.load_state_dict(checkpoint['model_state_dict'])\n","\n","print(model.state_dict())"],"metadata":{"id":"N03E9NGtXARx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_inputs = torch.tensor([[.20], [.34], [.57]])\n","\n","model.eval() # always use EVAL for fully trained models!\n","model(new_inputs.to(device))"],"metadata":{"id":"HBykxEc_XDGg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4XlI_WUvXHCO"},"execution_count":null,"outputs":[]}]}